{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step we need to import labrary and open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import statistics\n",
    "from collections import deque\n",
    "import pickle\n",
    "import os\n",
    "#os.chdir(\"C:\\\\Users\\\\user\\\\Documents\\\\HW5_ADM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RQ 1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HYPERLINKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the links and we store those pairs insiede a list first, and then we crated two dictionaries. First dictionary contains sources as a keys and destinations as values, instead the second one is the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki-topcats-reduced.txt\", 'r') as fl:\n",
    "    lista_1= []#list of link pairs \n",
    "    Nodes=[]#list of nodes\n",
    "    for line in fl:\n",
    "        i = [s.strip() for s in line.split(\"\\t\")]\n",
    "        lista_1.append(i)\n",
    "        Nodes.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes=set(Nodes)#we convert the list of nodes to set, because of the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_hl1 = defaultdict(list)#source to destination\n",
    "for k,v in lista_1:\n",
    "    dict_hl1[str(k)].append(str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_hl2 = defaultdict(list)#destination to source\n",
    "for k,v in lista_1:\n",
    "    dict_hl2[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sources is: 428957 and number of destinations is: 352518 \n",
      "The number of nodes of our graph is: 461193 \n",
      "The number of edges is: 2645247 \n",
      "The density is: 1.2436602635647606e-05 \n",
      "The average degree is: 5.735661642739591\n"
     ]
    }
   ],
   "source": [
    "print('The number of sources is: ' + str(len(dict_hl1)) + ' and number of destinations is: ' + str(len(dict_hl2)),\n",
    "      '\\nThe number of nodes of our graph is: ' + str(len(Nodes)),\n",
    "      '\\nThe number of edges is: ' + str(len(lista_1)),\n",
    "      '\\nThe density is: ' + str(len(lista_1)/(len(Nodes)*(len(Nodes)-1))),\n",
    "      '\\nThe average degree is: ' + str(len(lista_1)/len(Nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since number of sources and destinations are not the same we can conclude that it's a directed graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CATEGORIES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we opened the file taht contains categories' lists and we store it in a dicionary where the keys are the categories and values are list of articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-topcats-categories.txt', 'r') as fil:\n",
    "    lista_3 = []#it's a list of lists where the first value is tha name of categories and the others are the index of the articles\n",
    "    for line in fil:\n",
    "        i = [s.strip() for s in line.split()]\n",
    "        lista_3.append(i)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lst in lista_3:\n",
    "    lst[0] = lst[0][9:-1]#we just clean the name of categories just to be more easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_3 = {}#get a dictionary where the key is the category and value is the list of articols\n",
    "for v in lista_3:\n",
    "    dict_3[v[0]]=set(v[1:]).intersection(Nodes)#we take the intersection because we need only nodes that were mentioned in the reduced graph which we are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of categories to compare is: 29\n"
     ]
    }
   ],
   "source": [
    "dict_4 = {}#now we have a dictionary with only the categories with at least 3500 articols\n",
    "for k in dict_3:\n",
    "    #print(dict_3.)\n",
    "    if len(dict_3.get(k)) >= 3500:\n",
    "        dict_4[k] = dict_3.get(k)\n",
    "    else:\n",
    "        continue\n",
    "print('The number of categories to compare is: ' + str(len(dict_4)))# finally we have only 29 categories to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHORTEST PATH FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we defined our function to calculate the shortest path between two nodes. <br/>\n",
    "The function takes 2 values, the starting node and the ending node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_path(start, goal):\n",
    "\n",
    "    visited = []#nodes checked before reaching the goal\n",
    "    queue = deque([start])#list of nodes to be checked\n",
    "    distance = 0#that's our counter, to count the number of steps to reach the goal node\n",
    "        \n",
    "    while queue:\n",
    "        #we opened a loop, that it will be stopped only when the len of the queue will be 0.\n",
    "        #It will appen only when all the nodes connected to our starting point will be checked.\n",
    "        path = queue.popleft()#delete the first item from the queue that must be analysed.\n",
    "\n",
    "        if path not in visited:\n",
    "            #if the node is not in visited list, start to analyse it.\n",
    "            neighbours = dict_hl1[path]\n",
    "            #we take all the nodes directly connected to the node that we are analysing\n",
    "            \n",
    "                        \n",
    "            for neighbour in neighbours:\n",
    "                queue.append(neighbour)\n",
    "                #append all the neightbours to the queue because we need to check those in the future if \n",
    "                #we'll not find our goal in this time\n",
    "                    \n",
    "                distance +=1#add one to our counter\n",
    "                if distance >500:\n",
    "                # we put a condition, just to make our analyses faster, so if the distance is grater than 500 stop.\n",
    "                #We decided to do that because we 're looking for the shortest so we don't need to store \n",
    "                #all the possibile paths.\n",
    "                    return('path > 500')\n",
    "\n",
    "                elif neighbour == goal:\n",
    "                    #if the the neighbour == goal stop because you've fuound the goal node.\n",
    "                    return distance\n",
    "                \n",
    "            visited.append(path)\n",
    "            #store the analysed node inside the list because we don't wanna analyse it again.\n",
    "\n",
    "    else:\n",
    "        return ('No path found')\n",
    "        #if the queue si empty it means that there are no paths to reach the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implemented function to calculate all possible shortest paths between two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sh_p(c0,ci):\n",
    "    shortest_path_2 = defaultdict(list)#we defined a dictionary for storing values.The keys are nodes in input categories and values are paths to nodes to i-th category.\n",
    "    \n",
    "    for n in c0:# per each node of input category we started to calculate all the shortest paths to i-th category\n",
    "        for n_1 in ci:#we calculate the paths per each node of i-th category\n",
    "\n",
    "            if n == n_1:\n",
    "                shortest_path_2[n].append(0)\n",
    "                #if the source and destination are the same just add 0 and continue with the next node in i-th category.\n",
    "                break\n",
    "            elif len(shortest_path_2[n]) > 9:\n",
    "    #we decided to add another condition to make the compute faster.\n",
    "    #we decided to take only 9 possibile paths to node in i-th category, because we'll take only one of those,\n",
    "    #more precisely the shortest.\n",
    "                    break\n",
    "            else:\n",
    "                value = bfs_shortest_path(n, n_1)\n",
    "                # here we store the path inside a variable that it will be appended to the dictionary. \n",
    "                if value == 'No path found' or value == 'path > 500':\n",
    "                    #if the function gives a path greater than 500 or there is no path we store nothing\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    shortest_path_2[n].append(value)#if the function gives us a right path we'll store it into the dictionary.\n",
    "    return shortest_path_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we chose input category and than create a list with names of other categories. Then we executed functions for storing all possible pathes between input and other categories.\n",
    "During each iteration we stored every dictionary into a .pkl file, in this way we can run the function just one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0=dict_4['Association_football_goalkeepers']#input category\n",
    "\n",
    "slist=[i for i in dict_4.keys() if i != 'Association_football_goalkeepers']#list of all categories except input category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#that's our function to store every dictionary into a .pkl file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we store all the dictionaries categories' pairs into single files.\n",
    "#(if you don't wanna run it, you can find in the repository a zip file with all of them).\n",
    "for x in slist:\n",
    "    sh=calc_sh_p(C0,dict_4[x])#calculation of possible pathes\n",
    "    save_obj(sh,str(x))#storing to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#that's our function to load .pkl files\n",
    "def load_obj(name ):\n",
    "    with open('Dictionaries of category pairs/'+ name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RQ 2]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Block-Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the function to get the median from values of one dictionary.\n",
    "def get_median1(d_sh):\n",
    "    list_of_pathes=[]\n",
    "    for x in d_sh.keys():\n",
    "        if d_sh[x]==[]:\n",
    "            list_of_pathes.append(461194)#if there is no path we put this number because it's impossible to go through more than total number of nodes.\n",
    "        else:\n",
    "            list_of_pathes.extend(d_sh[x])#we extend our list if there are same pathes.\n",
    "    return statistics.median(list_of_pathes)#here we take the median of that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we calculate our ranking vector as dictionary where the keys are categories' names and values will be medians.\n",
    "all_rank_vec={}\n",
    "for x in slist:\n",
    "    namei=load_obj(x)#loading file that contains passible shortest pathes\n",
    "    ranki=get_median1(namei)#getting the median\n",
    "    all_rank_vec[x]=ranki#storing into dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Association_football_forwards', 157),\n",
       " ('Association_football_midfielders', 234.0),\n",
       " ('Living_people', 242.0),\n",
       " ('Place_of_birth_missing_(living_people)', 263),\n",
       " ('Year_of_birth_missing', 283),\n",
       " ('Year_of_birth_missing_(living_people)', 284.0),\n",
       " ('Year_of_death_missing', 287.0),\n",
       " ('English_footballers', 344),\n",
       " ('The_Football_League_players', 354.0),\n",
       " ('Association_football_defenders', 376.0),\n",
       " ('Harvard_University_alumni', 461194.0),\n",
       " ('Major_League_Baseball_pitchers', 461194.0),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       "  461194),\n",
       " ('Indian_films', 461194.0),\n",
       " ('Rivers_of_Romania', 461194),\n",
       " ('Main_Belt_asteroids', 461194),\n",
       " ('Asteroids_named_for_people', 461194),\n",
       " ('English-language_albums', 461194),\n",
       " ('British_films', 461194),\n",
       " ('English-language_films', 461194.0),\n",
       " ('American_films', 461194.0),\n",
       " ('People_from_New_York_City', 461194.0),\n",
       " ('American_television_actors', 461194.0),\n",
       " ('American_film_actors', 461194.0),\n",
       " ('Debut_albums', 461194),\n",
       " ('Black-and-white_films', 461194.0),\n",
       " ('American_military_personnel_of_World_War_II', 461194.0),\n",
       " ('Windows_games', 461194.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to sort our dictionary by values. \n",
    "Rank_list=sorted(all_rank_vec.items(), key=lambda kv: kv[1])\n",
    "Rank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input category we chose 'Association_football_goalkeepers', and from the output above it is clear that the closest categories are topics related to football and the farthest are topics related to other sports, films, science and history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Association_football_forwards',\n",
       " 'Association_football_midfielders',\n",
       " 'Living_people',\n",
       " 'Place_of_birth_missing_(living_people)',\n",
       " 'Year_of_birth_missing',\n",
       " 'Year_of_birth_missing_(living_people)',\n",
       " 'Year_of_death_missing',\n",
       " 'English_footballers',\n",
       " 'The_Football_League_players',\n",
       " 'Association_football_defenders',\n",
       " 'Harvard_University_alumni',\n",
       " 'Major_League_Baseball_pitchers',\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       " 'Indian_films',\n",
       " 'Rivers_of_Romania',\n",
       " 'Main_Belt_asteroids',\n",
       " 'Asteroids_named_for_people',\n",
       " 'English-language_albums',\n",
       " 'British_films',\n",
       " 'English-language_films',\n",
       " 'American_films',\n",
       " 'People_from_New_York_City',\n",
       " 'American_television_actors',\n",
       " 'American_film_actors',\n",
       " 'Debut_albums',\n",
       " 'Black-and-white_films',\n",
       " 'American_military_personnel_of_World_War_II',\n",
       " 'Windows_games']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting sorted names in order to do following calculations.\n",
    "ranked_names=[]\n",
    "for i in Rank_list:\n",
    "    ranked_names.append(i[0])\n",
    "ranked_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPUTING SUBGRAPHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sub(C00):# computing subgraph from input category.\n",
    "    C0_r={}\n",
    "    # we consider that at the beginnig all edges has 1 as weight\n",
    "    for i in C00:\n",
    "        if i in dict_hl2.keys():\n",
    "            rr=len(set(C00).intersection(set(dict_hl2[i])))#we take number of in-edges\n",
    "            if rr>0:\n",
    "                C0_r[i]=rr\n",
    "    return C0_r\n",
    "\n",
    "\n",
    "\n",
    "def transform_sub(C0_r,C00):#function for preprocessing to next calculation\n",
    "    WC0={}\n",
    "    t=[]\n",
    "    #we take each node in subgraph\n",
    "    for x in C0_r.keys():\n",
    "        if x in dict_hl1.keys():# if it sends edges\n",
    "            for i in set(dict_hl1[x]).intersection(C00):#for each node that gets edge we store the score of sending edge as weight\n",
    "                if i in WC0:# if node has several in-edges we store several weights\n",
    "                    t=WC0[i]\n",
    "                    t.append(C0_r[x])\n",
    "                    WC0[i]=t\n",
    "                else:\n",
    "                    WC0[i]=[C0_r[x]]\n",
    "    return WC0\n",
    "\n",
    "\n",
    "\n",
    "def sub_calc_2(C00,C1,WC0):# function to calculate score of nodes in subgraph with adding nodes from next category\n",
    "    C0_0=C00.union(C1)#adding nodes of the next category\n",
    "    C1_r={}\n",
    "    a=0#the default value for sum of in-edges from previous subgraph \n",
    "    for i in C0_0:\n",
    "        if i in dict_hl2.keys():#if node has in-edges\n",
    "            if i in WC0:#if node was in previous subgraph and had in-edges\n",
    "                a=sum(WC0[i])-len(WC0[i])# we take sum of weights ans we substract their count in order to avoid repetiton\n",
    "            val=a+len(set(dict_hl2[i]).intersection(C1))#taking the sum of weights\n",
    "            if val>0:\n",
    "                C1_r[i]=val#we store only values that are more than 0\n",
    "    return C1_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0=dict_4['Association_football_goalkeepers']# input category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[STEP 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub0=calc_sub(C0) # calculating scores for first subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3737"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C0) # number of nodes in fisrt subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(sub0,\"firstSubgraph\") # saving into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub0tramsf=transform_sub(sub0,C0) # getting weights of edges for the next computing bigger subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[STEP 2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_1=ranked_names[0] #name of the next category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Association_football_forwards'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub0_1=sub_calc_2(C0,set(dict_4[C_1]),sub0tramsf) #computing scores for subgraph that contains nodes \n",
    "#from the first graph and the first category of our block ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(sub0_1,\"secondSubgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0=C0.union(dict_4[C_1])# adding nodes from first category of block ranking to input category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8827"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C0) # number of nodes in the secod subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5198"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub0_1) # the number of nodes with in-edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[STEP 3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub0tramsf1=transform_sub(sub0_1,C0)# preproccessing of the second graph for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding the nodes from a category: 3 number of nodes: 14484\n",
      " number of nodes with in edges, that means they have a score: 9000\n",
      "adding the nodes from a category: 4 number of nodes: 349696\n",
      " number of nodes with in edges, that means they have a score: 262209\n",
      "adding the nodes from a category: 5 number of nodes: 349758\n",
      " number of nodes with in edges, that means they have a score: 262062\n",
      "adding the nodes from a category: 6 number of nodes: 353867\n",
      " number of nodes with in edges, that means they have a score: 264855\n",
      "adding the nodes from a category: 7 number of nodes: 353928\n",
      " number of nodes with in edges, that means they have a score: 265091\n",
      "adding the nodes from a category: 8 number of nodes: 356276\n",
      " number of nodes with in edges, that means they have a score: 266358\n",
      "adding the nodes from a category: 9 number of nodes: 357580\n",
      " number of nodes with in edges, that means they have a score: 267360\n",
      "adding the nodes from a category: 10 number of nodes: 357880\n",
      " number of nodes with in edges, that means they have a score: 267647\n",
      "adding the nodes from a category: 11 number of nodes: 357991\n",
      " number of nodes with in edges, that means they have a score: 267768\n",
      "adding the nodes from a category: 12 number of nodes: 360484\n",
      " number of nodes with in edges, that means they have a score: 269364\n",
      "adding the nodes from a category: 13 number of nodes: 362466\n",
      " number of nodes with in edges, that means they have a score: 270427\n",
      "adding the nodes from a category: 14 number of nodes: 367702\n",
      " number of nodes with in edges, that means they have a score: 275461\n",
      "adding the nodes from a category: 15 number of nodes: 373268\n",
      " number of nodes with in edges, that means they have a score: 280751\n",
      "adding the nodes from a category: 16 number of nodes: 380997\n",
      " number of nodes with in edges, that means they have a score: 288398\n",
      "adding the nodes from a category: 17 number of nodes: 392656\n",
      " number of nodes with in edges, that means they have a score: 292995\n",
      "adding the nodes from a category: 18 number of nodes: 393014\n",
      " number of nodes with in edges, that means they have a score: 293080\n",
      "adding the nodes from a category: 19 number of nodes: 397774\n",
      " number of nodes with in edges, that means they have a score: 297347\n",
      "adding the nodes from a category: 20 number of nodes: 402190\n",
      " number of nodes with in edges, that means they have a score: 301257\n",
      "adding the nodes from a category: 21 number of nodes: 421074\n",
      " number of nodes with in edges, that means they have a score: 319290\n",
      "adding the nodes from a category: 22 number of nodes: 425670\n",
      " number of nodes with in edges, that means they have a score: 323702\n",
      "adding the nodes from a category: 23 number of nodes: 427813\n",
      " number of nodes with in edges, that means they have a score: 325347\n",
      "adding the nodes from a category: 24 number of nodes: 429861\n",
      " number of nodes with in edges, that means they have a score: 327262\n",
      "adding the nodes from a category: 25 number of nodes: 432478\n",
      " number of nodes with in edges, that means they have a score: 329812\n",
      "adding the nodes from a category: 26 number of nodes: 439128\n",
      " number of nodes with in edges, that means they have a score: 335228\n",
      "adding the nodes from a category: 27 number of nodes: 442617\n",
      " number of nodes with in edges, that means they have a score: 338102\n",
      "adding the nodes from a category: 28 number of nodes: 445398\n",
      " number of nodes with in edges, that means they have a score: 339986\n",
      "adding the nodes from a category: 29 number of nodes: 449422\n",
      " number of nodes with in edges, that means they have a score: 343278\n"
     ]
    }
   ],
   "source": [
    "g0=sub0_1 #second subgraph\n",
    "gc0=sub0tramsf1# weights of edges from second subgraph\n",
    "c=2#that's the counter\n",
    "C0=set(C0)\n",
    "for x in ranked_names[1:]: # here should be our ranking vector\n",
    "    c=c+1\n",
    "    C11=set(dict_4[x])\n",
    "    sub=sub_calc_2(C0,C11,gc0)#getting subgraph\n",
    "    save_obj(sub,str(c)+\"_sub_\"+x)#saving into file\n",
    "    C0=C0.union(C11)#add nodes to set of nodes of the prevous graph\n",
    "    gc0=transform_sub(sub,C0)# getting weights of edges to compute scores for next graph\n",
    "    print(\"adding the nodes from a category: \"+ str(c)+\" number of nodes: \"+ str(len(C0))+\n",
    "         \"\\n number of nodes with in edges, that means they have a score: \"+str(len(sub))) # the number of nodes in subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print 30 nodes of the last subgraph, where there is the score per each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45819 The score of this node is: 31448106212962684043734196557061984449787\n",
      "1591276 The score of this node is: 375276843772144871886988461680898508276951\n",
      "1371112 The score of this node is: 375276843772144871886988461680898508276951\n",
      "724983 The score of this node is: 10631491339783285184273653253056319935295\n",
      "1196929 The score of this node is: 285974148439555534540491711566358016720336\n",
      "786730 The score of this node is: 17566296542326346503468122607592194095\n",
      "1602913 The score of this node is: 266872439095612402421049474129436882923315\n",
      "581239 The score of this node is: 53094465197549764440971449781549594879851\n",
      "668691 The score of this node is: 96313464409665255037263672270165045381202201\n",
      "1469821 The score of this node is: 5601327600969060894322021706152323426924862\n",
      "996339 The score of this node is: 17678423591677957728202351437362689737\n",
      "873959 The score of this node is: 362246557833235628918521160732855\n",
      "1273455 The score of this node is: 72745195579684980031610708435840854521\n",
      "1006941 The score of this node is: 3479492770959029652478691548452460598\n",
      "1494327 The score of this node is: 1420770531953230159253761062146183885547\n",
      "1535074 The score of this node is: 8373723529759984558663977387669139553842\n",
      "590627 The score of this node is: 36451547269457100664680248341653765456069\n",
      "1247426 The score of this node is: 821749086501660052434107823807225589411056\n",
      "903970 The score of this node is: 5\n",
      "1626291 The score of this node is: 5\n",
      "1542932 The score of this node is: 18525384429459568615201994992780594134898\n",
      "40445 The score of this node is: 18525384429459568615201994992780594134898\n",
      "541231 The score of this node is: 554751996105157146470509844115654699800\n",
      "1764151 The score of this node is: 872770477695492970912619234908585731006\n",
      "387567 The score of this node is: 6742004599150437927868143238113129504464379\n",
      "1441143 The score of this node is: 35378590339561486561356567650775074139300\n",
      "762484 The score of this node is: 2389803954344363778937526071650426097279605\n",
      "1548493 The score of this node is: 82562200219970315883631746073702765117866\n",
      "1642195 The score of this node is: 539086932603915404211048564315454596310713\n",
      "550822 The score of this node is: 64901589198797753116389116449275009499655\n"
     ]
    }
   ],
   "source": [
    "stop = 0\n",
    "for k,v in sub.items():\n",
    "    if stop == 30:\n",
    "        break\n",
    "    print(str(k) +' The score of this node is: ' + str(v))\n",
    "    stop += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
